package bigdata.spark_applications
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._


object Mymapdemo {
  
  def myinc(a:Int):Int={
    var b=a+1
    return b
  }
  
def main(args:Array[String]) = {

//We are creating a spark context object and also naming it
val conf = new SparkConf().setAppName("my map demo").setMaster("local")
val sc = new SparkContext(conf)

val myrdd=sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
val inc_rdd = myrdd.map(myinc)
inc_rdd.foreach(println)
System.in.read()
sc.stop()

  }

}